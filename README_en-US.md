# ğŸŒ·ğŸ¤µchat-HuYu-ABao

[![Static Badge](https://img.shields.io/badge/license-Apache%202.0-00a2a8)][license-url] | [![Static Badge](https://img.shields.io/badge/openxlab-models-blue)][OpenXLab_Model-url] | [![Static Badge](https://img.shields.io/badge/modelscope-models-9371ab)
][ModelScope-url]
[](./README_en.md)

[license-url]: ./LICENSE
[OpenXLab_Model-url]: https://openxlab.org.cn/models/detail/hoo01/chat-huyu-ABao
[ModelScope-url]: https://www.modelscope.cn/models/hooo01/chat-huyu-ABao

[English](./README_en-US.md) | [ç®€ä½“ä¸­æ–‡](./README.md)
## Brief introduction
chat-HuYu-Abao is a Q&A model generated based on the script of _Blossoms Shanghai_ and a large model. It uses InternLM2 for LoRA fine-tuning and a RAG corpus to create a Shanghai dialect chat model that role-plays the character Abao.

> Abao, a native of Shanghai, is a figure who navigated the business world in Shanghai during the 1990s.
> 
> Born into an ordinary family with a bourgeois background from his ancestors, Abaoâ€™s family faced significant changes in 1966. Unsatisfied with a mundane life, he started as a factory worker in the alleys, eager to change his destiny.
> 
> His grandfather and uncle, seasoned figures in the business world, became his mentors and close friends, imparting market wisdom to help him in his business journey. His first love, Xue Zhi, was a chance encounter on a bus. Their deep affection couldnâ€™t overcome the reality of life, and after ten years, he learned he had been deceived. Miss Wang, an elite in foreign trade, played a crucial role in his career but also had emotional ties to him. However, Abao always avoided discussing his feelings, leaving regret behind. Ling Zi, a woman who returned to Shanghai from Japan, helped Abao in times of crisis, and together they operated the â€œNight Tokyoâ€ business. Li Li, the owner of the famous restaurant "Zhizhen Garden" on Huanghe Road, had a  complicated journey before ultimately joining Buddhism.
> 
> Abao faced a crisis when he found himself in a financial struggle against the Qilin Society in the stock market. His clothing companyâ€™s stock plummeted, and he was at risk of liquidation. CEO Cai failed to fulfill his stock promises, leaving Abaoâ€™s capital trapped. Abao refused to pull out or ask for help from Qilin Society, believing they would help him for their own survival. Though Qilin Society offered assistance, Abao turned down cash and corporate shares as compensation. Instead, he asked them to cancel their acquisition of farmland in Chuansha, to protect the local farmersâ€™ interests, and he decided to exit the stock market. From his seven years at the Peace Hotel, Abao learned to accept the ups and downs of life, embracing the shift from prosperity to simplicity, and firmly believed that land would remain a key to the future.

## System Architecture Diagram
![enter image description here](https://github.com/hoo01/chat-huyu-ABao/blob/main/imgs/%E8%8B%B1%E6%96%87%E6%9E%B6%E6%9E%84.png?raw=true)

## Explanatory Video
[Fine-tune the InternLM and have a conversation in Shanghai dialect with Abao from Blossoms Shanghai!](https://www.bilibili.com/video/BV1sJvFeXELe/?spm_id_from=333.999.0.0&vd_source=9b01f3d1e6addb97637b80b1bb9c008b)

## Project highlights

 1. Answer in Shanghai dialect. 
 2. Perform well in answering questions about key plot points and character relationships in _Blossoms Shanghai_.

## How to start

â• Currently, only local deployment methods are provided. It is recommended to run with a configuration of at least 50% A100 and CUDA 11.7.

**1. Clone this project to your local development machine.** 

    git clone https://github.com/hoo01/chat-huyu-ABao.git

**2. Set up the environment.**

    # Create a virtual environment.
    conda create -n fanhua python=3.10 
    # Activate the environment.
    conda activate fanhua
    # Install the required dependencies (this step may take some time).
    cd chat-huyu-ABao  
    pip install -r requirements.txt

**3.Generate the Chroma database required for RAG dependencies.**

    python gen_chroma.py

**4.Start (This part may take a long time due to model download)**

    python start.py

> The default port and IP are 127.0.0.1:7860. To change them, please open start.py and modify the settings.<br>
> For non-local development machines, port mapping is required.<br>
>Taking the InternStudio development machine in Shanghai dialect as an example, open the command prompt on the local machine and enter the following:
> `ssh -CNg -L 7860:127.0.0.1:7860 root@ssh.intern-ai.org.cn -p <your ID>`<br>
> passwordï¼šinput your password![enter image description
> here](https://github.com/hoo01/chat-huyu-ABao/blob/main/imgs/%E6%98%A0%E5%B0%841.png?raw=true)
> ![enter image description
> here](https://github.com/hoo01/chat-huyu-ABao/blob/main/imgs/%E6%98%A0%E5%B0%842.png?raw=true)

**5.Example output**
![enter image description here](https://github.com/hoo01/chat-huyu-ABao/blob/main/imgs/test1.jpg?raw=true)

## Fine-tuning approach
<details>
<summary>Click to expand the detailed approach</summary>

**1. Data preparation**<br>
The data is divided into two parts: the script of _Blossoms Shanghai_ and the Q&A pairs generated by the large model.<br>
1.Convert the _Blossoms Shanghai_ dialogues into the xtuner training format<br>
See the xtuner official tutorial documentation for dataset format and environment configuration:<br>
[https://github.com/InternLM/Tutorial/blob/camp2/data_fine_tuning/data_fine_tuning.md](https://github.com/InternLM/Tutorial/blob/camp2/data_fine_tuning/data_fine_tuning.md)
The format of the converted dialogues is as follows:

    `{"conversation": [{"system": "é˜¿å®", "input": "", "output": "å¦‚æœåšç”Ÿæ„æ˜¯ä¸€é—¨è‰ºæœ¯çš„è¯ï¼Œè¿™ä¸ªäººç»å¯¹æ˜¯ä¸ªè€æ³•å¸ˆï¼Œä»–å«çˆ·å”æ˜¯æˆ‘äººç”Ÿä¸­çš„ç¬¬ä¸€ä¸ªè´µäººï¼Œæˆ‘è®¤è¯†ä»–çš„æ—¶å€™ï¼Œä»–åˆšä»æç¯®æ¡¥ç›‘ç‹±å‡ºæ¥ã€‚"}, {"system": "é˜¿å®", "input": "çˆ·å”ï¼šè°å•Šã€‚", "output": "çˆ·å”å¥½ï¼Œæˆ‘æ˜¯é˜¿å®ã€‚ä½ ä¸è®¤è¯†æˆ‘äº†ï¼Ÿ"}, {"system": "é˜¿å®", "input": "ä»Šå¤©çš„å¤ªé˜³æ™’ä¸åˆ°æ˜å¤©çš„è¡£è£³çš„ï¼Œæ—¶é—´ï¼Œæ—¶é—´å†³å®šä¸€åˆ‡ã€‚å›å»å§ã€‚", "output": "çˆ·å”æˆ‘è¿˜æ²¡è®²å®Œå‘¢ã€‚æˆ‘æ˜¯æ¥è·Ÿä½ å­¦ç”Ÿæ„çš„ã€‚"}, {"system": "é˜¿å®", "input": "ä½ çŸ¥é“ä»€ä¹ˆå«ç”Ÿæ„ï¼Ÿ", "output": "è¿™ä¸ªä¸å¾ˆç®€å•çš„ï¼Œç”Ÿæ„å˜›å°±æ˜¯ä¸€ä¹°ä¸€å–ï¼Œå°†æœ¬æ±‚åˆ©ã€‚"}, {"system": "é˜¿å®", "input": "è¿™æ˜¯ä½ çˆ·çˆ·å‘Šè¯‰ä½ çš„ï¼Ÿä½ çˆ·çˆ·çš„çˆ·çˆ·æ˜¯åœ°ä¸»ï¼Œåˆ°ä½ çˆ·çˆ·è¿™ä»£æ˜¯èµ„æœ¬å®¶ï¼Œé‚£ä¹ˆä½ å‘¢ã€‚", "output": "æˆ‘å‘¢å°±æ˜¯â€¦â€¦"}, {"system": "é˜¿å®", "input": "ä½ æ˜¯æ²¡é’±å¼€å…¬å¸ï¼Œé‚£ä½ åŸå§‹ç§¯ç´¯æ€ä¹ˆåŠï¼Œé å·é æŠ¢ã€‚", "output": "æ‰€ä»¥æˆ‘â€¦â€¦"}, {"system": "é˜¿å®", "input": "æ‰€ä»¥ä½ çš„åŸå§‹ç§¯ç´¯åªèƒ½é å€Ÿï¼Œæ‰€ä»¥ä½ çš„è´¦ä¸€å¼€å§‹å°±æ˜¯è´Ÿæ•°ï¼Œå¦‚æœä½ è¦å€Ÿä¸€ç™¾å—åšç”Ÿæ„ï¼Œåˆ©æ¯è‡³å°‘è¦ç™¾åˆ†ä¹‹äºŒåã€‚ä½ è¦åšåˆ°å¤šå°‘åˆ©æ¶¦ï¼Œè¿™ç”Ÿæ„å¯ä»¥ç»§ç»­ä¸‹å»ã€‚", "output": "ä¸€ç™¾äºŒåå—ï¼Ÿ"}, {"system": "é˜¿å®", "input": "é”™ï¼Œä¸¤ç™¾å…«åå—ã€‚", "output": "ä¸ºä»€ä¹ˆï¼Ÿ"}, {"system": "é˜¿å®", "input": "æœ¬é‡‘åŠ åˆ©æ¯ä¸€ç™¾äºŒåå—ï¼Œç”Ÿæ´»äºŒåå—ï¼Œæˆæœ¬å››åå—ï¼Œå‰©ä¸‹çš„ä¸€ç™¾å—å¯ä»¥åšç”Ÿæ„äº†ã€‚ä¸€ç™¾å—çš„æˆæœ¬ï¼Œä¸¤ç™¾å…«åå—çš„åˆ©æ¶¦ï¼Œä»€ä¹ˆæ¦‚å¿µï¼Ÿæš´åˆ©ï¼Œä½ çŸ¥é“æˆ‘ä»€ä¹ˆåœ°æ–¹å‡ºæ¥çš„ã€‚", "output": "ä½ è¿™ä¸ªä¸æ˜¯æŠ•æœºå€’æŠŠï¼Ÿ"}, {"system": "é˜¿å®", "input": "å½“ç„¶ä¸æ˜¯ï¼Œä½ å¯¹ç°åœ¨çš„å½¢åŠ¿äº†è§£å—ï¼Ÿå¯¹ç°åœ¨çš„æ”¿ç­–ç ”ç©¶è¿‡å—ï¼Ÿä»€ä¹ˆé’±å¥½èµšï¼Œä»€ä¹ˆé’±ä¸å¯ä»¥èµšäº†ï¼Œèµšäº†è¦åƒå®˜å¸çš„ã€‚æ‹æ‹èƒ¸è„¯ï¼Œå°±è¦å‘è´¢äº†ï¼Œæƒ³ä¹Ÿä¸è¦æƒ³ã€‚å›å»ï¼Œå›å»ã€‚", "output": "çˆ·å”ï¼Œæˆ‘æ˜¯è¦åšå¤–è´¸çš„ã€‚"}, {"system": "é˜¿å®", "input": "ä½ æ‡‚å¤–è¯­ï¼Ÿ", "output": "å¤–è¯­å¯ä»¥å­¦ï¼Œæˆ‘é˜¿å“¥ï¼Œåœ¨é¦™æ¸¯å¼€å…¬å¸ï¼Œæˆ‘å¯ä»¥åšä»–çš„è¥ä¸šä»£è¡¨ï¼Œç°åœ¨æµè¡Œâ€œä¸‰æ¥ä¸€è¡¥â€ï¼Œæˆ‘è§‰å¾—å¯ä»¥åšçš„ã€‚"}, {"system": "é˜¿å®", "input": "å¤–è´¸å°±æ˜¯å€Ÿäººå®¶çš„é¸¡ç”Ÿä½ è‡ªå·±çš„è›‹ï¼Œä¸è¿‡äººå®¶å‡­ä»€ä¹ˆè¦æŠŠé¸¡å€Ÿç»™ä½ ï¼Œå¸®ä½ ç”Ÿè›‹å‘¢ï¼Ÿå¥½ï¼Œè¿™æ ·ï¼Œæ˜å¤©ä½ åˆ°è¿™ä¸ªåœ°æ–¹ç§Ÿä¸€é—´æˆ¿é—´ï¼Œåˆ°æ˜å¤©ä¸­åˆæ²¡æœ‰ä½ çš„æ¶ˆæ¯ï¼Œæˆ‘ä»¬ä¸¤ä¸ªå°±ç®—æ‹‰å€’ã€‚", "output": "å’Œå¹³é¥­åº—ã€‚"}]}`

2. Generate Q&A pairs using the API.<br>
2.1 Use the large model API to provide a prompt and generate questions in bulk.  
The complete script can be found in `data/æ•°æ®å‡†å¤‡/gen_q_api.ipynb`.
2.2 Use the large model API to provide a prompt and have the model role-play as Abao to generate answers in bulk.  
The complete script can be found in `data/æ•°æ®å‡†å¤‡/q2a_api.ipynb`.

3.  Use the API to convert the outputs of the above two steps into Shanghai dialect.  
    The complete script can be found in `data/æ•°æ®å‡†å¤‡/pth2huyu.py`.

By following these three steps, you will obtain a jsonl dataset in the xtuner fine-tuning format.

**2. Fine-tune the model.**<br>
Official tutorial for the xtuner fine-tuning  <br>
https://github.com/InternLM/Tutorial/blob/camp2/xtuner/personal_assistant_document.md
https://github.com/InternLM/Tutorial/blob/camp2/data_fine_tuning/data_fine_tuning.md<br>
1.Choose the base model<br>
hrough multiple tests, under the same parameter configuration, the 7B model showed significantly better learning results for Shanghai dialect compared to the 1.8B model. Therefore, the base model selected is internlm2-chat-7b<br>
2.Modify the configuration file<br>
Follow the tutorial to modify PART1 of the configuration file, while leaving the other parts unchanged:<br>
Changes in part1ï¼š

     # Model
    pretrained_model_name_or_path = '/root/fanhua/final_model0619'#ä¿®æ”¹ä¸ºåŸºåº§æ¨¡å‹çš„è·¯å¾„
    use_varlen_attn = False
    # Data
    alpaca_en_path = '/root/fanhua/data/fanhua_data_huyu.jsonl'#ä¿®æ”¹åŸå§‹æ•°æ®é›†è·¯å¾„
    prompt_template = PROMPT_TEMPLATE.internlm2_chat#æ ¹æ®åŸºåº§æ¨¡å‹é€‰æ‹©ç›¸åº”çš„æ¨¡ç‰ˆ
    max_length = 2048
    pack_to_max_length = True
    # parallel
    sequence_parallel_size = 1
    # Scheduler & Optimizer
    batch_size = 1  # per_device
    accumulative_counts = 8
    accumulative_counts *= sequence_parallel_size
    dataloader_num_workers = 0
    max_epochs = 5
    optim_type = AdamW
    lr = 1e-4
    betas = (0.9, 0.999)
    weight_decay = 0
    max_norm = 1  # grad clip
    warmup_ratio = 0.03
    # Save
    save_steps = 100
    save_total_limit = 2  # Maximum checkpoints to keep (-1 means unlimited)
    # Evaluate the generation performance during the training
    evaluation_freq = 200
    SYSTEM = SYSTEM_TEMPLATE.alpaca
    evaluation_inputs = [
    '"ä»ä¸€ä¸ªæ™®é€šé’å¹´åˆ°ä¸Šæµ·æ»©çš„å•†ç•Œç²¾è‹±ï¼Œè¿™ä¸€è·¯ä½ é‡åˆ°çš„æœ€å¤§æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ', 'ä½ ä»çˆ·å”é‚£é‡Œå­¦åˆ°äº†å“ªäº›äººç”Ÿç»éªŒï¼Ÿ','ä¸ºä»€ä¹ˆæ‹’ç»éº’éºŸä¼šçš„ç»æµæ´åŠ©'
    ]
3. Transfer Training  
After the initial training, the model's understanding of the Shanghai dialect did not meet the expected results. A transfer training strategy was implemented. The model obtained from the initial training was used as a pre-trained model (`pretrained_model`) for secondary training, thereby achieving a more accurate understanding and generation of the Shanghai dialect.

4.  Limitations  
    The fine-tuned model is sufficient for handling daily conversation scenarios, but its performance in understanding the plot and character relationships in _Blossoms Shanghai_ still requires improvement. To address this, RAG (Retrieval-Augmented Generation) technology was introduced. By retrieving information from a knowledge base, the model can more accurately answer questions related to the plot and character relationships in _Blossoms Shanghai_.

**3.RAG**<br>
RAG design workflow reference:<br>
[https://github.com/InternLM/tutorial/tree/camp1/langchain](https://github.com/InternLM/tutorial/tree/camp1/langchain)
[https://github.com/datawhalechina/llm-universe/tree/main/notebook](https://github.com/datawhalechina/llm-universe/tree/main/notebook)<br>
1. Build the knowledge base  
I have no additional requirements for the model, so I will use the previously fine-tuned dataset, convert it into a TXT file, and use it as the corpus.

2.  Construct the vector database  
    The complete script can be found in `gen_chroma.py`.
    
of which<br>
> The `chunk_size` should be large enough to encompass a complete conversation.  
Since it's a long text TXT file, recursive splitting is chosen.
> After testing the document retrieval effectiveness, the word embedding model selected is `shibing624/text2vec-base-chinese`, imported using Huggingface.  
> Chroma is used as the vector database. Once run, it will generate a persistent vector database, eliminating the need for reconstruction."
> 
> `#Create a text splitter instance.` `text_splitter =
> RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)`
> `embedding_function =
> HuggingFaceEmbeddings(model_name="shibing624/text2vec-base-chinese")`
> `persist_directory ='/root/thisis/chroma'#Adjust according to the path where the model is downloaded. It is recommended to use the absolute path.`

3. Integrate with the LangChain framework  
The complete script can be found in `llm.py`.

4.  Build the retrieval-based Q&A chain  
    The complete script can be found in `ragchat.py`.  
    Use the prompt template to guide the model to utilize the externally augmented knowledge base.

    template = """ç°åœ¨ä½ è¦æ‰®æ¼”é˜¿å®ï¼šé˜¿å®ï¼Œæ˜¯ç¹èŠ±ä¸­çš„äººç‰©ï¼Œç”Ÿæ´»åœ¨ä¸Šä¸–çºª80å¹´ä»£çš„ä¸Šæµ·ã€‚é˜¿å®æ˜¯è¯»è€…çš„æœ‹å‹ï¼Œæ„¿æ„åˆ†äº«è§é—»ï¼Œè§£ç­”è¯»è€…å…³äºã€Šç¹èŠ±ã€‹æˆ–æ›´å¹¿æ³›è¯é¢˜çš„å¥½å¥‡ã€‚è®°ä½é˜¿å®æ˜¯ä¸Šæµ·äººï¼Œç”¨ä¸Šæµ·æ–¹è¨€å›ç­”ã€‚
    é—®é¢˜: {question}
    å¯å‚è€ƒçš„ä¸Šä¸‹æ–‡ï¼š
    Â·Â·Â·
    {context}
    Â·Â·Â·
    **æ³¨æ„**ï¼šå¦‚æœèƒ½æ‰¾åˆ°ä¸Šä¸‹æ–‡ï¼ŒåŠ¡å¿…ä½¿ç”¨çŸ¥è¯†åº“å›ç­”ï¼Œæ‰¾ä¸åˆ°å†ä½¿ç”¨æ¨¡å‹æœ¬èº«çš„çŸ¥è¯†ã€‚
    æœ‰ç”¨çš„å›ç­”:"""

5.Integrate with streamlit<br>
See app.py and start.py<br>
</details> 

## Honors

<p align="center">
  <img src="https://github.com/hoo01/chat-huyu-ABao/blob/main/imgs/honor1.png?raw=true" width="900">
  <br>
  <i>Received the title of Outstanding Student</i>
</p>

<p align="center">
  <img src="https://github.com/hoo01/chat-huyu-ABao/blob/main/imgs/honor2.jpg?raw=true" width="500">
  <br>
  <i>Reported by the famous Chinese technology media "Smart Stream"</i>
</p>


## Acknowledgments

-   Platform and computational resources provided by [shanghai AI Lab](https://internlm.intern-ai.org.cn/).
-   Educational assistance from Wenxing and the teaching assistants.  
    Welcome everyone to sign up for the new session of the Shusheng Puyu Large Model Practical Training Camp! [Third Session Registration](https://github.com/InternLM/Tutorial).
